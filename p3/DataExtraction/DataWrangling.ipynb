{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    return df.replace(np.nan,'', regex=True).T.to_dict()\n",
    "\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        station_data = csv_reader.next()\n",
    "        header = csv_reader.next()\n",
    "        name = station_data[1]\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "    \n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "    \n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mintime': (2013, 2, 3, 4, 0, 0), 'maxtime': (2013, 8, 13, 17, 0, 0), 'avgcoast': 10976.933460679751, 'maxvalue': 18779.025510000003, 'minvalue': 6602.113898999982}\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "       \n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    cols = sheet.col_values(1)\n",
    "    cols.pop(0)\n",
    "    \n",
    "    max_val = max(cols)\n",
    "    min_val = min(cols)\n",
    "    \n",
    "    max_time = xlrd.xldate_as_tuple(sheet.cell_value(cols.index(max_val)+1,0),0)\n",
    "    min_time = xlrd.xldate_as_tuple(sheet.cell_value(cols.index(min_val)+1,0),0)\n",
    "    data = {\n",
    "            'maxtime': max_time,\n",
    "            'maxvalue': max_val,\n",
    "            'mintime': min_time,\n",
    "            'minvalue': min_val,\n",
    "            'avgcoast': sum(cols)/len(cols)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    print data\n",
    "\n",
    "    \n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = []\n",
    "    for col in range(1,sheet.ncols):\n",
    "        station = sheet.cell_value(0,col)\n",
    "        if station == 'ERCOT':\n",
    "            continue\n",
    "        cols = sheet.col_values(col)\n",
    "        cols.pop(0)\n",
    "        max_val = max(cols)\n",
    "    \n",
    "        year, month, day, hour, minute, second = xlrd.xldate_as_tuple(sheet.cell_value(cols.index(max_val)+1,0),0)\n",
    "        row = {\n",
    "            'Station': station,\n",
    "            'Year': year,\n",
    "            'Month': month,\n",
    "            'Day': day,\n",
    "            'Hour': hour,\n",
    "            'Max Load':max_val\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        fieldnames = ['Station','Year','Month','Day','Hour','Max Load']\n",
    "        w = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='|')\n",
    "        w.writeheader()\n",
    "        w.writerows(data)\n",
    "    \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AQueen&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Queen\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Queen\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"2db42837-c832-3c27-b4a3-08198f75693c\", \n",
      "                \"name\": \"Japan\", \n",
      "                \"sort-name\": \"Japan\"\n",
      "            }, \n",
      "            \"country\": \"JP\", \n",
      "            \"disambiguation\": \"character, voiced by \\u677f\\u91ce\\u53cb\\u7f8e / Itano Tomomi\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"420ca290-76c5-41af-999e-564d7c71f1a7\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Queen\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"related-akb48\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"kamen rider w\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Character\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"US rapper\", \n",
      "            \"id\": \"5eecaf18-02ec-47af-a4f2-7831db373419\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": \"2011\", \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Queen + Adam Lambert\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Queen + Adam Lambert\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"UK rock group\", \n",
      "            \"id\": \"0383dadf-2a4e-4d10-a46a-e9e041da8eb3\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1970-06-27\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Queen\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 9, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"progressive rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -2, \n",
      "                    \"name\": \"70s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -2, \n",
      "                    \"name\": \"80s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -2, \n",
      "                    \"name\": \"90s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"pop-rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 3, \n",
      "                    \"name\": \"british\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"uk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"band\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 3, \n",
      "                    \"name\": \"hard rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"britannique\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"disco\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"glam rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"queen family\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"art rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"english\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"united kingdom\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"queen\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"classic pop and rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"platinum\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"pop/rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"80's\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": -1, \n",
      "                    \"name\": \"90's\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"70's\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"langham 1 studio bbc\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"kind of magic\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Acic Queen\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Acic Queen\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"id\": \"8a03436b-2120-4d04-9f3b-9e18753ba8b7\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Acid Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Acid Queen\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"c6303af1-d66e-4e8a-930e-fe1d6ea5b781\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Majeeda\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Majeeda\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"85752fda-13c4-31a3-bee5-0e5cb1f51dad\", \n",
      "                \"name\": \"Germany\", \n",
      "                \"sort-name\": \"Germany\"\n",
      "            }, \n",
      "            \"country\": \"DE\", \n",
      "            \"disambiguation\": \"German duo Ina M\\u00fcller & Edda Schnittgard\", \n",
      "            \"id\": \"fd31549e-6a5f-4ff8-94fa-5a68638952fc\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1994\", \n",
      "                \"end\": \"2005\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Queen Bee\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Bee\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"8d25fda6-e74e-45ac-9695-7e3e6a7d0fac\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"B.B. Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"B.B. Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"08310658-51eb-3801-80de-5a0739207115\", \n",
      "                \"name\": \"France\", \n",
      "                \"sort-name\": \"France\"\n",
      "            }, \n",
      "            \"country\": \"FR\", \n",
      "            \"id\": \"ea57c459-a6dd-40ea-b489-03b7a912be13\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Sister Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Sister Queen\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"9a86679d-7661-4633-b1b0-76a69922458c\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Monica Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen, Monica\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"country\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Queen Elisabeth\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Queen Elisabeth\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"British band with Julian Cope\", \n",
      "            \"id\": \"1e067199-f4cd-477b-9a87-7170e7ed4cf7\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Elizabeth\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Elizabeth\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"f45984aa-f1ca-41d8-898a-af07340ae244\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Pearly Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Pearly Queen\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"funk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"rhythm and blues\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"latin\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"latin funk\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"b9de990e-2ece-42dd-827c-30c4ca4a99a4\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen V\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen V\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"08310658-51eb-3801-80de-5a0739207115\", \n",
      "                \"name\": \"France\", \n",
      "                \"sort-name\": \"France\"\n",
      "            }, \n",
      "            \"country\": \"FR\", \n",
      "            \"id\": \"ee2d368e-3580-46d3-9de7-84110254b6c0\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1977\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Samantha\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Samantha\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"eceb4f3d-95d9-4566-9fe2-a307fbcc3a4b\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Black Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Black Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"3906cf32-00a7-32df-93cc-4710c5f5a542\", \n",
      "                \"name\": \"Puerto Rico\", \n",
      "                \"sort-name\": \"Puerto Rico\"\n",
      "            }, \n",
      "            \"country\": \"PR\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"ff06ecc0-72a5-4a81-a253-d840d9aefb32\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Ivy Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Ivy Queen\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"reggaet\\u00f3n\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"a71b0d32-7752-49e9-8594-2247ad6ac12c\", \n",
      "                \"name\": \"Brooklyn\", \n",
      "                \"sort-name\": \"Brooklyn\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"e6476e4f-ad4c-4e29-bd20-0cd622ffe441\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1973\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Pen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Pen, Queen\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"fbd2bc49-d2dd-410d-a870-950e56f02d5b\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Filter Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Filter Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"8636f35d-0e6c-447e-a77f-718a85124b80\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Stampede Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Stampede Queen\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"26069161-b64d-4021-9c7e-7686dae8ffb4\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Pork Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Pork Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"8eb2fc14-e41d-4c02-b3f2-4a4fa3d88664\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Witch Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Witch Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"b46cb0af-039b-4966-9c3e-43a29823778e\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Jenny Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen, Jenny\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"a4bb4530-95a8-4877-b4d7-44ada42846dc\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Disco Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Disco Queen\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"8636578b-9eeb-4bc5-844f-6816890d230c\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Omega\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Omega\"\n",
      "        }, \n",
      "        {\n",
      "            \"gender\": \"male\", \n",
      "            \"id\": \"ff1f59d7-4675-4029-9833-816b1d1bcf9d\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Size Queen\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Size Queen\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"e74250aa-8f69-426c-9a10-944e53ac5dba\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Queen Mecca\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Queen Mecca\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 268, \n",
      "    \"created\": \"2017-01-31T10:46:08.213Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"disambiguation\": \"US rapper\", \n",
      "    \"id\": \"5eecaf18-02ec-47af-a4f2-7831db373419\", \n",
      "    \"life-span\": {\n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"Queen\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"Queen\"\n",
      "}\n",
      "requesting http://musicbrainz.org/ws/2/artist/5eecaf18-02ec-47af-a4f2-7831db373419?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": \"\", \n",
      "  \"country\": \"US\", \n",
      "  \"date\": \"2015-04-21\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"e53a4efc-bbe2-4422-958f-720920dce51b\", \n",
      "  \"packaging\": \"None\", \n",
      "  \"packaging-id\": \"119eba76-b343-3e02-a292-f0f00644bb9b\", \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"US\"\n",
      "        ], \n",
      "        \"name\": \"United States\", \n",
      "        \"sort-name\": \"United States\"\n",
      "      }, \n",
      "      \"date\": \"2015-04-21\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"But I Still Love You\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "But I Still Love You\n",
      "Unadorned\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "def main():\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Queen\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)      \n",
    "    titles = []\n",
    "    urls =[]\n",
    "    for article in data:\n",
    "        titles.append({article['section']:article['title']})\n",
    "        for media in article['media']:\n",
    "            for media_meta in media['media-metadata']:\n",
    "                if media_meta['format'] == 'Standard Thumbnail':\n",
    "                    urls.append(media_meta['url'])\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert len(urls) == 30\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                'fnm': author.find('fnm').text,\n",
    "                'snm': author.find('snm').text,\n",
    "                'email': author.find('email').text,\n",
    "                'insr': []\n",
    "        }\n",
    "        for insr in author.findall('insr'):\n",
    "            data['insr'].append(insr.get('iid'))\n",
    "        \n",
    "        authors.append(data)\n",
    "    return authors\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"rb\") as html:\n",
    "        soup = BeautifulSoup(html)\n",
    "        data['eventvalidation'] = soup.find('input', id='__EVENTVALIDATION')['value']\n",
    "        data['viewstate'] = soup.find('input', id='__VIEWSTATE')['value']\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        options = soup.find(id='CarrierList').find_all('option')\n",
    "        for option in options:\n",
    "            if 'All' not in option['value']:\n",
    "                data.append(option['value'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': airport,\n",
    "                          'CarrierList': carrier,\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")  \n",
    "        options = soup.find(id='AirportList').find_all('option')\n",
    "        for option in options:\n",
    "            if 'All' not in option['value']:\n",
    "                data.append(option['value'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    courier, airport = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        trs = soup.find(id=\"DataGrid1\").find_all('tr',{'class':'dataTDRight'})\n",
    "        \n",
    "        for tr in trs:\n",
    "            tds = tr.find_all('td')\n",
    "            if tds[0].getText().isdigit() and tds[1].getText().isdigit():\n",
    "                info = {'courier': courier, 'airport': airport}\n",
    "                info['year'] = int(tds[0].getText())\n",
    "                info['month'] = int(tds[1].getText())\n",
    "                info['flights'] = {'domestic': int(tds[2].getText().replace(\",\",\"\")),\n",
    "                           'international': int(tds[3].getText().replace(\",\",\"\"))}\n",
    "                data.append(info)\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "    \n",
    "def test2():\n",
    "    data = process_file('FL-ATL.html')\n",
    "\n",
    "test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "    n = 0\n",
    "    with open(filename) as f:  \n",
    "        for line in f:\n",
    "            if line.startswith(\"<?xml\"):  \n",
    "                if n > 0:\n",
    "                    out.close()   \n",
    "                out = open(\"{}-{}\".format(filename, n), 'wb')  \n",
    "                n += 1           \n",
    "            out.write(line)\n",
    "            \n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
