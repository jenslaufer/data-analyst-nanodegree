---
title: "Identifying Fraud from Enron Emails and Financial Data"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---



```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
library(stringr)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
raw_df = read.csv('raw_data.csv')
without_nans_df = read.csv('cleaned_data1.csv')
cleaned_df = read.csv('cleaned_data2.csv')
kbest_results_df = read.csv('kbest.csv')
metrics_df = read.csv('metrics.csv')

nulls <- raw_df %>%
  gather(-name, -email_address, -poi, key = "var", value = "val")  %>%
  group_by(var) %>%
  summarise(number = dim(raw_df)[1] - sum(val == 'NaN') -1) %>%
  arrange(desc(number))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
outlier_plots <- function(df){
df %>%
  gather(-name, -email_address, -poi, key = "var", value = "value")  %>%
  group_by(var) %>%
  mutate(max = max(value)) %>%
  ggplot(aes(x =poi, y=value)) +
   # geom_point(position='jitter', alpha=0.2, color=c('blue')) +
    geom_boxplot() +
    geom_text(aes(label=ifelse(value > .6 * max, as.character(name), '')))+
    facet_wrap(~var, scales = "free")
  
}


metric_plots <- function(metrics_df, filter.tag){
  metrics_df %>% 
  filter(tag==filter.tag) %>%
  gather(-classifier, -datatype, -tag, -features, key = "var", value = "val")  %>%
  filter(var != 'accuracy') %>%
  ggplot(aes(x = datatype, y=val)) +
    geom_bar(stat='identity', width=0.05, fill=c('blue'), alpha=0.4) +
    geom_text(aes(label=round(val,3))) +
    geom_hline(aes(yintercept=0.3), linetype='dotted', color='red', size=1) +
    facet_grid(var ~ classifier) 
  
}


features_metric_plots <- function(metrics_df){
  metrics_df %>% 
  filter(tag!='pre') %>%
  filter(tag!='tuned') %>%
  gather(-classifier, -datatype, -tag, -features, key = "var", value = "val")  %>%
  filter(var != 'accuracy') %>%
  ggplot(aes(x = datatype, y=val)) +
    geom_bar(stat='identity', width=0.05, fill=c('blue'), alpha=0.4) +
    geom_text(aes(label=round(val,3))) +
    geom_hline(aes(yintercept=0.3), linetype='dotted', color='red', size=1) +
    facet_wrap(tag ~ var) 
  
}
```



by Jens Laufer, 02/08/2017



## Introduction


Enron was formed in 1985 by a merge of Houston Natural Gas Co and InterNorth 
Inc. Kenneth Lay became Enron's first CEO and Chairman and transformed the 
company into a energy trader and supplier. Deregulation of the energy market and 
an aggressive company policy fired the growth of the company. By 2000 the 
company was one of the biggest companies in the US and a Wallstreet darling. 
Behind the scenes financial losses of the company were hidden behind a network 
of companies with the help of the CEOs Kenneth Lay and Jeff Skilling and the 
CFO Andrew Fastow and others. 
By the end of 2001 the company declared bankruptcy all of a 
sudden to the public. 
The U.S. Securities and Exchange Commission (SEC) began an investigation and 
unhided the biggest case of account fraud in the US history.
Kenneth Lay, Jeff Skilling, Andrew Fastow and several other excutives were later 
sentenced to prison.

The goal of this project is to develop a machine learning model to classify, 
if a person is a so called person of interest (POI), who was involved in fraud.

For the project a dataset with email and financial data of 146 excutives is used. 
The original dataset can be downloaded from 
(https://www.cs.cmu.edu/enron/enron_mail_20150507.tgz) and was enriched with  
financial data.


## Data Exploration

The data set consists of `r dim(raw_df)[1]` observations and 
`r dim(raw_df)[2]-1` features.

There are 
`r dim(subset(raw_df, poi == 'True'))[1]` POI
and 
`r dim(subset(raw_df, poi == 'False'))[1]` non POIs in the dataset.



### Cleaning the Data



```{r echo=FALSE, message=FALSE, warning=FALSE}
nulls %>%
  ggplot(aes(x=var, y=number)) +
  geom_bar(stat='identity', alpha=0.5, fill=c('blue')) +
  scale_x_discrete(limits=rev(as.factor(nulls$var))) +
  labs(y='Number', x='Feature', title='Number of available features') +
  coord_flip()
```




Occurences of NaN values were checked. It can be seen that for some 
features not that many data points are available. Especially 'loan_advances'
just have 3 data points. This has to be kept in mind for the feature 
selection.

For financial data the NaN values were replaced with 0.


### Outliers


Outliers need to be removed from the dataset. As fraud detection is 
the goal of the project this has to be done with caution as some POIs have 
outliers in their financial data. These outliers are
essential and cannot be thrown away as they characterize a POI. 
So just undesired outliers can be removed.

```{r echo=FALSE, fig.height=8, fig.width=16, message=FALSE, warning=FALSE}
outlier_plots(without_nans_df) 
```

From the figure we can see a outlier for the POI 'TOTAL'. 
These seems more a sum rather than a person.

The following outliers were removed:

'TOTAL' is removed, as it is the sum of a financial
feature data points.

'THE TRAVEL AGENCY IN THE PARK' is removed as it is not a person.

'LOCKHART EUGENE E' is removed as all features are NaN.

'KAMINSKI WINCENTY J': Non POI with outlier on 'from_messages'

'BHATNAGAR SANJAY': Non POI with outlier on 'from_messages'

'FREVERT MARK A': Non POI with outlier on 'deferrall_payments'

'LAVORATO JOHN J': Non POI with outlier on 'bonus'

'MARTIN AMANDA K': Non POI with outlier on 'long_term_incentives'

'WHITE JR THOMAS E': Non POI with outlier on 'restricted_stock'

'KEAN STEVEN J': Non POI with outlier on 'from_messages'

'ECHOLS JOHN B': Non POI with outlier on 'long_term_incentives'




```{r echo=FALSE, fig.height=8, fig.width=16, message=FALSE, warning=FALSE}
outlier_plots(cleaned_df) 
```
The figure shows the distribution of the dataset after cleaning the 'bad' 
outliers. As mentioned before everything seem ok now, there are only "desired"
outliers.



## Feature Engineering


3 new features were created:

   - total_financial_benefits: Sum of 'salary', 'bonus', 'total_stock_value', 
   and 'exercised_stock_options'. 
   
   - message_to_poi_ratio: Ratio a person  sends
   messages to POI
   
   - message_from_poi_ratio:  Ratio a person receives  messages from POI

The message ratios were created as it seems possible that POIs 
communicate more likely with each other.

'total_financial_benefits' was created as it summarizes all 
financial benefits a person gets. 


Missing values of 'from_messages' and 'to_messages' were replaced 
with the mean of these fields over the whole data set to not tap 
into division by zero problems.

Feature scaling was performed after the creation of message fraction 
features and before the creation of 'total_financial_benefits' 
as some algorithms like logistic regression and SVM will not 
work properly without scaling.  
It was done before the fraction calculation to avoid division by 0 problems.



## Feature Selection


```{r echo=FALSE, message=FALSE, warning=FALSE}
  kbest_arranged <- kbest_results_df %>%
                     arrange(desc(score))
  
  kbest_arranged %>%
  ggplot(aes(x=var, y=score)) +
  geom_bar(stat='identity', alpha=0.5, fill=c('red')) +
  scale_x_discrete(limits=rev(as.factor(kbest_arranged$var))) +
  labs(y='Score', x='Feature', size='Number of available features', 
       title='KBest Scores') +
  coord_flip()

```

As mentioned before 'loan_advances' just have 3 data points. 
It was therefore removed before a SelectKBest Analysis.

A univariate feature exploration was then performed with SelectKBest. 
As it can be seen from the figure the new created feature
'total_financial_benefits' has the highest score. 
The new feature 'fraction_of_messages_to_poi'
seems also promising, whereas the 'fraction_of_messages_to_poi' 
has a low score.

'to_messages' and 'from_messages' have a low score and are also 
used for the calculation of the new features, so they can be removed.

As the total_financial_benefits is the sum of 'salary', 'bonus', 
'total_stock_value', 'exercised_stock_options'
these features are removed to avoid colinearity.



### Quality of features


```{r warning=FALSE, echo=FALSE, fig.height=8, fig.width=16, message=FALSE}
#features_metric_plots(metrics_df)
axes_limits = c('only_message_from_poi_ratio',
                'only_message_to_poi_ratio', 
                'only_total_financial_benefits', 
                'no_new_features',
                'all_new_features',
                'top_10_feature_list_from_kbest',
                'top_5_feature_list_from_kbest',
                'top_15_feature_list_from_kbest')


metrics_df %>% 
  filter(tag!='pre') %>%
  filter(tag!='tuned') %>%
  gather(-classifier, -datatype, -tag, -features, key = "var", value = "val")  %>%
  filter(var != 'accuracy') %>%
  ggplot(aes(x = tag, y=val)) +
    geom_bar(aes(fill=tag), stat='identity', width=0.05, alpha=0.4) +
    geom_text(aes(label=round(val,3))) +
    scale_x_discrete(limits=axes_limits,breaks=c('')) +
    scale_fill_discrete(limits =axes_limits) +
    geom_hline(aes(yintercept=0.3), linetype='dotted', color='red', size=1) +
    facet_grid(datatype ~ var) 
  
```



## Classifier Evaluation


The selected features were applied to 3 different Machine Learning Algorithms:

  - Decision Tree
     
  - Gaussian Naive Base
     
  - Logistic Regression
    
For a first run the default parameters were used to get an impression 
how these algorithms perform without optimizations. 

80% of the data is used for training/cross validation and 20% for testing.


#### Definition of optimization/validation metrics


Within the project recall, precision and f1 is used for optimization and validation. 
The metrics are defined as follows:



   - **recall**:   Out of all POIs which are predicted as positive, how many are truly belong to positive case. 
   A higher recall score would mean less false negatives, which means in this case if there are POIs in the datset, an algorithm has good chance to identify them.
   - **precision**: Out of all POIs that are truly positive, how many are correctly classified as positive. 
   A higher precision 
   score would mean less false positives.  A high precision means POIs identified by an algorithm tended to be correct.
  - **f1**:  It is a harmonic mean of recall and precision. 
  It is a metric that combines recall and precision into single metric.
  
  

The goal was to get a recall and a precision of at least 0.3.
The f1 is used as well, as it combines both recall and precision into one metric.


```{r warning=FALSE, echo=FALSE, fig.height=10, fig.width=20, message=FALSE}
metric_plots(metrics_df, 'pre')
```

#### DecisionTreeClassifier

For the DecisionTreeClassifier the values for recall 
and precision are touching the goal of 0.3 without optimization on the
cross validation set. The results on the test set are similiar.


#### GaussianNB

The Gaussian Naive Baise Classifier has a low precision  and a high 
recall on the cross validation set.
The metrics on the test set are better than on the cross validation set. The recall is 1,
so the model was able to find any true POI in the set, but classified also many non POI
incorrectly as POIs reflected by the low precision.


#### Logistic Regression

The Logistic Regression has a relatively high precision on the cross validation set.
The recall is over the goal of 0.3. When it comes to the validation set, the model
fails as both precision and recall are 0.



## Classifier Tuning 

The models of the unoptimized classifiers that are used before 
fail on the test set. Therefore tuning is needed.

Machine learning algorithms are parameterized and
modfication of the parameters influence the 
outcome of the learning process. The objective
of algorithm tuning is to find the best parameters
for the income features. The search problem can be automated by applying different parameters from a parameter grid.

For tuning GridSearchCV in combination with pipelines is used. The pipeline has 2 stages, one for
further reducing the dimension of the features and one for estimator optimization. The f1 score
is used to select the best estimator. f1 is used as it reflects both the recall and precision and
it's balancing out these two metrics equally. Another option would have been using f2 or higher,
if the focus are higher recall scores, which might make sense for this kind of estimator, as we
want to identify true POIs correctly.


#### Reducing dimension stage

The goal of this stage is to further reduce the dimension of the feature set. Therefore princial 
component analysis (PCA) is used which 
is reducing linear dimensionality with Singular Value Decomposition. 

A second method of feature reduction is recursive feature elimination and 
cross-validated selection (RFECV), which ranks the best combination and number of features.

Different parameters for
the optimizations of the reducers are systematically evaluted by GridSearchCV.


#### Estimator optimization stage

In this stage different parameters are systematically applied to GaussianNB, LogisticRegression and 
DecisionTreeClassifier. 




```{r warning=FALSE, echo=FALSE, fig.height=10, fig.width=20, message=FALSE}
metric_plots(metrics_df, 'tuned')
```

Based on the f1 score, which was used by the pipeline to rate the classifiers
the best classifier is a Logistic Regression 
classifier and a PCA step before applying the features to the classifier.


The optimized classifier (in combination with PCA) has a high recall and a 
precision over the target value of 0.3. It's ability to identify true POIs correctly is good, as it's recall metric is high. 

The classifier and Feature reducer were finally validated on the test set: The specification was met also on the test set, although the score are dropping a little bit too much, which might be a sign of overfitting.


## Validation Strategy


Validation or testing the model is the final step to proof, if a model after training is able to generalize. For validation several metrics can be used, like recall, precision and f1 as explained before.

There are three main scenarios which can occur:

   - Underfitting: The model cannot capture the underlying trend in the data. The model has a poor predictive power. This is reflected in a low value on the used metric both on the training and test set.
   - Overfitting: The model describes the noise instead of the underlying relationship. This is reflected in  a high value of the used validation metric on the training data and a strong drop of the metric on the test data.
   - "Perfect fitting": The model has a strong preditive power both on the test and training set, which is reflected in a high value of the used metric on both sets. This scenario is the goal of any optimization of a machine learning model.

For validation the inital data set is splitted into a set for training and another one for test.
A third set can be used, which is called cross validation data set. This is used for optimization of the model and
the test set is just used for a final check. In cases when not so many data points available the data set can be used for cross validation by splitting it several times into a training set and test set.

Within this project the whole data set is splitted into a trainig set and test set. The test is 20% of the data.

The training set is used for cross validation by splitting up the training set 1000 times 
with a test set size of 20%. The metrics are calculated based on the 1000 splits. 
StratifiedShuffleSplit was used to ensure the split is 
random, but the ratio training to test data is about the same.


The cross validation data set is used for optimizations, the test set just 
for a final test/proof if the results are sufficient.




## Conclusion

Within this project machine learning techniques were used to identify person of interest POI in Enron fraud. The difficulty was, that there are only a few data points (146) and the data set is unbalanced, as there are just 18 POIs in the data set.

First the data was cleaned by replacing NaN with 0 for financial data and with mean for email to_messages and from_messages fields. Then outliers were removed.

Three new features were created: One for summary of financial benefits and two for interaction of persons with POIs. The features were scaled with MinMaxScaling.

Some of the features were then excluded from the feature list by performing a SelectKBest. Most of the features were kept, as in the step for tuning the machine learning algoritms a further feature reduction step was planed.

Three machine learning algorithms were then trained and compared:  GaussianNB, DecisionTreeClassifier and Logistic Regression. The LogisticRegression performed the best out the three without any optimization, but all classifier
failed on the test set.

The final step was tuning the algorithm. Before the tuning of the algorithms parameters another feature reduction
step was performed: Principal Component Analysis (PCA) and recursive feature elimination and 
cross-validated selection (RFECV) were optimized by GridSearchCV in this step. The final step was optimizing different
parameters on the algorithms, by comparing them systematically

LogisticRegression with a principal component analysis performed the best:

```{r warning=FALSE, echo=FALSE, message=FALSE}
summary <-   metrics_df %>% filter(tag=='tuned') 
```
   - Features: `r summary$features[1]`

   - PCA: copy=True, iterated_power=7, n_components=0.6, random_state=None, 
   svd_solver=‘auto’, tol=0.0, whiten=False
   - LogisticRegression: C=0.05, class_weight=‘balanced’,
dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=‘ovr’, n_jobs=1,
penalty=‘l2’, random_state=None, solver=‘liblinear’, tol=0.1, verbose=0, warm_start=False



```{r warning=FALSE, echo=FALSE, message=FALSE}
summary <- summary[ , !(names(summary) %in% c('tag', 'classifier','accuracy', 'features'))]
```

```{r warning=FALSE, echo=FALSE, message=FALSE}
summary %>%
  kable(col.names=c('f1', 'precision','recall', 'Dataset'))
```

The classifier has a high recall, which means it is good in identifying actual 
POIs.





