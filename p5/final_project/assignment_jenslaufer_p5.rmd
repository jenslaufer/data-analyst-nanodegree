---
title: "Identifying Fraud from Enron Emails and Financial Data"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---



```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
library(stringr)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
raw_df = read.csv('raw_data.csv')
without_nans_df = read.csv('cleaned_data1.csv')
cleaned_df = read.csv('cleaned_data2.csv')
kbest_results_df = read.csv('kbest.csv')
metrics_df = read.csv('metrics.csv')

nulls <- raw_df %>%
  gather(-name, -email_address, -poi, key = "var", value = "val")  %>%
  group_by(var) %>%
  summarise(number = dim(raw_df)[1] - sum(val == 'NaN') -1) %>%
  arrange(desc(number))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
outlier_plots <- function(df){
df %>%
  gather(-name, -email_address, -poi, key = "var", value = "value")  %>%
  group_by(var) %>%
  mutate(max = max(value)) %>%
  ggplot(aes(x =poi, y=value)) +
   # geom_point(position='jitter', alpha=0.2, color=c('blue')) +
    geom_boxplot() +
    geom_text(aes(label=ifelse(value > .6 * max, as.character(name), '')))+
    facet_wrap(~var, scales = "free")
  
}


metric_plots <- function(metrics_df, filter.tag){
  metrics_df %>% 
  filter(tag==filter.tag) %>%
  gather(-classifier, -datatype, -tag, key = "var", value = "val")  %>%
  ggplot(aes(x = datatype, y=val)) +
    geom_bar(stat='identity', width=0.05, fill=c('blue'), alpha=0.4) +
    geom_text(aes(label=round(val,2))) +
    geom_hline(aes(yintercept=0.3), linetype='dotted', color='red', size=1) +
    facet_grid(var ~ classifier) 
  
}
```



by Jens Laufer, 02/08/2017



## Introduction


Enron was formed in 1985 by a merge of Houston Natural Gas Co and InterNorth 
Inc. Kenneth Lay became Enron's first CEO and Chairman and transformed the 
company into a energy trader and supplier. Deregulation of the energy market and 
an aggressive company policy fired the growth of the company. By 2000 the 
company was one of the biggest companies in the US and a Wallstreet darling. 
Behind the scenes financial losses of the company were hidden behind a network 
of companies with the help of the CEOs Kenneth Lay and Jeff Skilling and the 
CFO Andrew Fastow and others. 
By the end of 2001 the company declared bankruptcy all of a 
sudden to the public. 
The U.S. Securities and Exchange Commission (SEC) began an investigation and 
unhided the biggest case of account fraud in the US history.
Kenneth Lay, Jeff Skilling, Andrew Fastow and several other excutives were later 
sentenced to prison.

The goal of this study is to develop a machine learning model to classify, if a
person is a so called person of interest (POI), who was involved in fraud.

For the study a dataset with email and financial data of 146 excutives is used. 
The original dataset can be downloaded from 
(https://www.cs.cmu.edu/enron/enron_mail_20150507.tgz) and was enriched with  
financial data.


## Data Exploration

The data set consists of `r dim(raw_df)[1]` observations and 
`r dim(raw_df)[2]-1` features.

There are 
`r dim(subset(raw_df, poi == 'True'))[1]` POI
and 
`r dim(subset(raw_df, poi == 'False'))[1]` non POIs in the dataset.



### Cleaning the Data



```{r echo=FALSE, message=FALSE, warning=FALSE}
nulls %>%
  ggplot(aes(x=var, y=number)) +
  geom_bar(stat='identity', alpha=0.5, fill=c('blue')) +
  scale_x_discrete(limits=rev(as.factor(nulls$var))) +
  labs(y='Number', x='Feature', title='Number of available features') +
  coord_flip()
```




Occurences of NaN values were checked. It can be seen that for some 
features not that many data points are available. Especially 'loan_advances'
just have 3 data points. This has to be kept in mind for the feature 
selection.

For financial data the NaN values were replaced with 0.


### Outliers


Outliers need to be removed from the dataset. As fraud detection is 
the goal of the study this has to be done with cautious as some POIs have 
outliers in their financial data. These outliers are
essential and cannot be thrown away as they characterize a POI. 
So just undesired outliers can be removed.

```{r echo=FALSE, fig.height=8, fig.width=16, message=FALSE, warning=FALSE}
outlier_plots(without_nans_df) 
```

From the figure we can see a outlier for the POI 'TOTAL'. These seems more a sum rather than a person.

The following outliers were removed:

'TOTAL' is removed, as it is the sum of a financial
feature data points.

'THE TRAVEL AGENCY IN THE PARK' is removed as it is not a person.

'LOCKHART EUGENE E' is removed as all features are NaN.

'KAMINSKI WINCENTY J': Non POI with outlier on 'from_messages'

'BHATNAGAR SANJAY': Non POI with outlier on 'from_messages'

'FREVERT MARK A': Non POI with outlier on 'deferrall_payments'

'LAVORATO JOHN J': Non POI with outlier on 'bonus'

'MARTIN AMANDA K': Non POI with outlier on 'long_term_incentives'

'WHITE JR THOMAS E': Non POI with outlier on 'restricted_stock'

'KEAN STEVEN J': Non POI with outlier on 'from_messages'

'ECHOLS JOHN B': Non POI with outlier on 'long_term_incentives'




```{r echo=FALSE, fig.height=8, fig.width=16, message=FALSE, warning=FALSE}
outlier_plots(cleaned_df) 
```
The figure shows the distribution of the dataset after cleaning the 'bad' 
outliers. As mentioned before everything seem ok now, there are only "desired"
outliers.



## Feature Engineering


3 new features were created:

   - total_financial_benefits: Sum of 'salary', 'bonus', 'total_stock_value', 
   and 'exercised_stock_options'. 
   
   - message_to_poi_ratio: Ratio a person  sends
   messages to POI
   
   - message_from_poi_ratio:  Ratio a person receives  messages from POI

The message ratios were created as it seems possible that POIs communicate more likely with each other.

'total_financial_benefits' was created as it summarizes all financial benefits a person gets. 


Missing values of 'from_messages' and 'to_messages' were replaced with the mean of these fields over the whole data set to not tap into division by zero problems.

Feature scaling was performed after the creation of message fraction 
features and before the creation of 'total_financial_benefits' as some algorithms like logistic regression and SVM will not work properly without scaling.  It was done before the fraction calculation to avoid division by 0 problems.



## Feature Selection



```{r echo=FALSE, message=FALSE, warning=FALSE}
  kbest_arranged <- kbest_results_df %>%
                     arrange(desc(score))
  
  kbest_arranged %>%
  ggplot(aes(x=var, y=score)) +
  geom_bar(stat='identity', alpha=0.5, fill=c('red')) +
  scale_x_discrete(limits=rev(as.factor(kbest_arranged$var))) +
  labs(y='Score', x='Feature', size='Number of available features', 
       title='KBest Scores') +
  coord_flip()

```

As mentioned before 'loan_advances' just have 3 data points. It was therefore removed before a SelectKBest Analysis.

A univariate feature exploration was then performed with SelectKBest. As it can be seen from the figure the new created feature
'total_financial_benefits' has the highest score. The new feature 'fraction_of_messages_to_poi'
seems also promising, whereas the 'fraction_of_messages_to_poi' has a low score.

'to_messages' and 'from_messages' have a low score and are also used for the calculation of the new features, so they can be removed.

As the total_financial_benefits is the sum of 'salary', 'bonus', 'total_stock_value', 'exercised_stock_options'
these features are removed to avoid colinearity.



## Classifier


The selected features were applied to 3 different Machine Learning Algorithms:

  - Decision Tree
     
  - Gaussian Naive Base
     
  - Logistic Regression
    
For a first run the default parameters were used to get an impression how these algorithms perform without optimizations. 

The data set is splitted into a set for training and a set for test. The test set is 30% of the 
training data.

The training set is used for cross validation by splitting the training set 500 times 
with a test set size of 30%. The metrics are calculated based on the 500 splits. 
StratifiedShuffleSplit was used to ensure the split is 
random, but the ratio training to test data is about the same.

The cross validation data set is used for optimizations, the test set just 
for a final test/proof if the results are sufficient.

The goal was to get a recall and a precision of at least 0.3 and an accuracy as high as possible.
The f1 is used to evaluate the combination of recall and precision.


```{r warning=FALSE, echo=FALSE, fig.height=10, fig.width=20, message=FALSE}
metric_plots(metrics_df, 'pre')
```

#### DecisionTreeClassifier

The DecisionTreeClassifier reaches quiet a high accuracy on the cross validation set. The values for recall 
and precision are over the goal of 0.3 without optimization. But when the test set is applied it can be seen
that recall and precision are 0, which means that the classifier was not able to identify any POIs correctly
(number of true positives is 0)


#### GaussianNB

The Gaussian Naive Baise Classifier has a low accuracy on the cross validation sets. The precision is low and the 
recall is quiet high, which means the  classifier was quiet accurate in finding all the positive samples. The
values on the test set are even better. The classifier was able to identify find all positive samples (recall=1),
but it's ability of finding the negative samples was limited. It seems strange that the classifier performs
actually better on the test set than on the cross validation sets.


#### Logistic Regression

The Logistic Regression classifier had a high accuracy on the cross validation sets, but it's ability to
identify true POIs correctly (low recall) was limited, while it was quiet accurate in identifying non POIs.
The results are very similiar on the test set.



### Tuning 


For tuning GridSearchCV in combination with pipelines is used. The pipeline has 2 stages, one for
further reducing the dimension of the features and one for estimator optimization. The f1 score
is used to select the best estimator. f1 is used as it reflects both the recall and precision and
it's balancing out these two metrics equally. Another option would have been using f2 or higher,
if the focus are higher recall scores, which might make sense for this kind of estimator, as we
want to identify true POIs correctly.


#### Reducing dimension stage

The goal of this stage is to further reduce the dimension of the feature set. Therefore princial 
component analysis (PCA) is used which 
is reducing linear dimensionality with Singular Value Decomposition. 

A second method of feature reduction is recursive feature elimination and 
cross-validated selection (RFECV), which ranks the best combination and number of features.


#### Estimator optimization stage

In this stage different parameters are applied to GaussianNB, LogisticRegression and 
DecisionTreeClassifier. 



```{r warning=FALSE, echo=FALSE, fig.height=10, fig.width=20, message=FALSE}
metric_plots(metrics_df, 'tuned')
```

Based on the f1 score, the best classifier is a Logistic Regression 
classifier and a PCA step before the classifier.


The optimized classifier (in combination with PCA) has good accuracy on the cross validation sets. It's ability to identify POIs is high, as it's recall metric is high. The values of recall and precision are higher than the goal
of 0.3.  

The classifier and Feature reducer were finally validated on the test set: The specification was met also on the test set, although the score are dropping a little bit too much, which might be a sign of overfitting.



### Conclusion





